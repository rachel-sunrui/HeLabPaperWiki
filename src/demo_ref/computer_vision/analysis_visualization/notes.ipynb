{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@article{Berkes:2006el,\n",
    "author = {Berkes, Pietro and Wiskott, Laurenz},\n",
    "title = {{On the Analysis and Interpretation of Inhomogeneous Quadratic Forms as Receptive Fields}},\n",
    "journal = {Neural Computation},\n",
    "year = {2006},\n",
    "volume = {18},\n",
    "number = {8},\n",
    "pages = {1868--1895},\n",
    "month = aug,\n",
    "annote = {I only read it upt to Section 5 (inlcuded), as I care more about visualization, not about statistical test.\n",
    "\n",
    "essential contributions.\n",
    "\n",
    "1. General methods to solve Eq. (4.1). That is, maximizing/minimizing a inhomogeneous quadratic form (which can model complex V1 cells), under a norm constraint.\n",
    "\n",
    "2. Eq. (5.5). given optimal stimulus, find the change of firing rate along all directions orthgonal to the direction of optimal stimulus (Eq. (5.10)). For a demo of this, see\n",
    "    * [Visualization of optimal stimuli and invariances \n",
    "for Tiled Convolutional Neural Networks](http://ai.stanford.edu/{\\textasciitilde}quocle/TCNNweb/)\n",
    "    * [original code](http://people.brandeis.edu/~berkes/software/qforms-tk/index.html)\n",
    "    * [Yimeng's implementation](https://github.com/leelabcnbc/tang-paper-2017/blob/master/neuron_fitting/debug/debug_hessian_visualization_complex_cell.ipynb)\n",
    "\n",
    "There are some errors and points note taking in the paper about the method.\n",
    "\n",
    "1. Above (5.8), we actually need to compute the null space of $x^+$ (N-1 vectors orthogonal to $x^+$). Check Quoc Le and my implementation on how to do this. Doing Gram-Schmidt on $x^+$ plus $e_1$ through $e_{N-1}$ doesn't necessarily give the correct result.\n",
    "2. As $x^+$ is local maxima, then you would expect along all directions, (5.5) gives you negative values. This is true, but eigenvalues of Eq. (5.8) are not all negative. Instead, you need to add back the offset term in Eq. (5.5). Then all offsetted eigenvalues are negative. Least negative ones are most invariant, and vice versa. Check original implmentation and Yimeng's implementation (`best_variance_and_invariance_directions `).\n",
    "3. Eq. (5.8) is simply computing the eigenvalues of H, along directions othorgonal to $x^+$. That is, they paramterize $w$ with $Bx$, $B$ being a $N$ by $N-1$ basis matrix, and $x$ being a $N-1$ vector. This justifies why using Eq. (5.9) to recover the original space.\n",
    "\n",
    "\n",
    "Some caveats\n",
    "\n",
    "For this to work, you need to 1) really find the optimal stimulus (at least some local maxima), and 2) compute the hessian correctly. Both are pretty difficult for a really complex neuron, say CNN.\n",
    "\n",
    "1) is either not very accurate, or it gives noise like input (from my experience, images that excite a aritifical neuron most are mostly noise like).\n",
    "2) is either theoretically impossbie (see <https://github.com/leelabcnbc/tang-paper-2017/blob/master/neuron_fitting/debug/debug_hessian_old_plus_adam_vs_lbfgs.ipynb>), or it takes so long to compute (say, using TensorFlow).\n",
    "\n",
    "Yimeng found that this method doesn't work well with really complex neurons. See <https://github.com/leelabcnbc/tang-paper-2017/blob/master/neuron_fitting/debug/cnn_fitting_debug_nonOT_visualize.ipynb>\n",
    "\n",
    "In the paper, they are not analyzing some fitted V1 cells. Instead, they are analzying some artificial units learned by Slow Feature Analysis. Check Section 2. According to Deep Learning book, SFA has closed form solution, so it can't be too complicated.\n",
    "\n",
    "\n",
    "Last paragraph of Section 4.  optimal x might not make sense or be relevant.\n",
    "\n",
    "> Note that although $x^+$ is the stimulus that elicits the strongest response in the function, it does not necessarily mean that it is representative of the class of stimuli that give the most important contribution to its output. This depends on the distribution of the input vectors.\n",
    "\n",
    "},\n",
    "publisher = {MIT Press 238 Main St., Suite 500, Cambridge, MA 02142-1046 USA journals-info@mit.edu},\n",
    "keywords = {V1},\n",
    "doi = {10.1162/neco.2006.18.8.1868},\n",
    "language = {English},\n",
    "read = {Yes},\n",
    "rating = {4},\n",
    "date-added = {2017-05-29T17:19:56GMT},\n",
    "date-modified = {2017-06-12T15:26:03GMT},\n",
    "abstract = {In this letter, we introduce some mathematical and numerical tools to analyze and interpret inhomogeneous quadratic forms. The resulting characterization is in some aspects similar to that given by experimental studies of cortical cells, making it particularly suitable for application to second-order approximations and theoretical models of physiological receptive fields. We first discuss two ways of analyzing a quadratic form by visualizing the coefficients of its quadratic and linear term directly and by considering the eigenvectors of its quadratic term. We then present an algorithm to compute the optimal excitatory and inhibitory stimuli{\\textemdash}those that maximize and minimize the considered quadratic form, respectively, given a fixed energy constraint. The analysis of the optimal stimuli is completed by considering their invariances, which are the transformations to which the quadratic form is most insensitive, and by introducing a test to determine which of these are statistically significant. Next we prop...},\n",
    "url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.8.1868},\n",
    "local-url = {file://localhost/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2006/Berkes/Neural%20Computation%202006%20Berkes.pdf},\n",
    "file = {{Neural Computation 2006 Berkes.pdf:/Users/yimengzh/Documents/Papers3_revised/Library.papers3/Articles/2006/Berkes/Neural Computation 2006 Berkes.pdf:application/pdf}},\n",
    "uri = {\\url{papers3://publication/doi/10.1162/neco.2006.18.8.1868}}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
